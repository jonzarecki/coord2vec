{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T13:56:46.514937Z",
     "start_time": "2020-04-19T13:56:46.482555Z"
    }
   },
   "source": [
    "# Tile2Vec embedded features for house pricing regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:13:04.347772Z",
     "start_time": "2020-04-19T14:13:04.344574Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_modules_to_path(modules):\n",
    "    for module in modules:\n",
    "        if module not in sys.path:\n",
    "            sys.path.append(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:13:06.604711Z",
     "start_time": "2020-04-19T14:13:04.596144Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from time import time\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "modules = [os.path.abspath('../../../..')]\n",
    "add_modules_to_path(modules)\n",
    "\n",
    "\n",
    "from coord2vec.Noam_Adir.Adir.tile2vec.pre_trained_model.tilenet import make_tilenet\n",
    "from coord2vec.Noam_Adir.Adir.tile2vec.pre_trained_model.resnet import ResNet18\n",
    "from coord2vec.Noam_Adir.Adir.tile2vec.tiles_data.data import get_fast_the_tiles, show_some_tiles_images\n",
    "# from coord2vec.Noam_Adir.Adir.tile2vec.tile2vec_utils import *\n",
    "# from coord2vec.Noam_Adir.pipeline.base_pipeline import *\n",
    "from coord2vec.Noam_Adir.manhattan.pipeline import init_pipeline, fit_and_score_models_on_datasets\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "logs_dir = '/mnt/adir_logs/tile2vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:13:08.606355Z",
     "start_time": "2020-04-19T14:13:06.606407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up model\n",
    "cuda = torch.cuda.is_available()\n",
    "# tilenet = make_tilenet(in_channels=in_channels, z_dim=z_dim)\n",
    "# Use old model for now\n",
    "tilenet = ResNet18(in_channels=4, z_dim=512, tile_size=100)\n",
    "if cuda: tilenet.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:13:08.724419Z",
     "start_time": "2020-04-19T14:13:08.608172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameters\n",
    "model_fn = 'pre_trained_model/naip_trained.ckpt'\n",
    "checkpoint = torch.load(model_fn)\n",
    "tilenet.load_state_dict(checkpoint)\n",
    "tilenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:13:09.622594Z",
     "start_time": "2020-04-19T14:13:08.725820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "n_tiles = 16000\n",
    "tiles = get_fast_the_tiles()\n",
    "tiles = tiles[:n_tiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:13:22.967821Z",
     "start_time": "2020-04-19T14:13:09.624197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 94., 108., 117.,   0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles.shape\n",
    "# infrared = np.random.randint(0, 256, size=(n_tiles, 224, 224, 1))\n",
    "infrared = np.zeros((n_tiles, 224, 224, 1))\n",
    "tiles_with_infrared = np.concatenate([tiles, infrared], axis=3)\n",
    "tiles_with_infrared[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T13:58:04.679969Z",
     "start_time": "2020-04-19T13:57:58.270845Z"
    }
   },
   "source": [
    "## Show some tiles with or without histogram equalization and with red mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Natural tiles:')\n",
    "show_some_tiles_images(tiles, nrows=2, ncols=3, n_tiles=n_tiles)\n",
    "print('Now with histogram equalization:')\n",
    "show_some_tiles_images(tiles, nrows=2, ncols=3, n_tiles=n_tiles, with_hist_eq=True)\n",
    "print('Now with red mark:')\n",
    "show_some_tiles_images(tiles, nrows=2, ncols=3, n_tiles=n_tiles, with_red_mark=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Manhattan tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T14:13:27.121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b067cac4d9411e83336e4246cfc571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Embedding tiles', max=640.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Embed tiles\n",
    "# embeddings = np.zeros((n_tiles, 2048))\n",
    "# for idx in range(n_tiles):\n",
    "#     tile = tiles_with_infrared[idx]\n",
    "#     # Rearrange to PyTorch order\n",
    "#     tile = np.moveaxis(tile, -1, 0)\n",
    "#     tile = np.expand_dims(tile, axis=0)\n",
    "#     # Scale to [0, 1]\n",
    "#     tile = tile / 255\n",
    "#     # Embed tile\n",
    "#     tile = torch.from_numpy(tile).float()\n",
    "#     tile = Variable(tile)\n",
    "#     if cuda: tile = tile.cuda(1)\n",
    "#     z = tilenet.encode(tile)\n",
    "#     if cuda: z = z.cpu()\n",
    "#     z = z.data.numpy()\n",
    "#     embeddings[idx] = z\n",
    "\n",
    "# Embed tiles\n",
    "batch_size = 25\n",
    "z_lst = []\n",
    "for i in tqdm(range(0, tiles_with_infrared.shape[0], batch_size), desc='Embedding tiles', unit='batch'):\n",
    "    tile = tiles_with_infrared[i: i + batch_size]\n",
    "    tiles_torch = torch.from_numpy(np.moveaxis(tile, -1, 1) / 255).float()\n",
    "    tiles_torch = Variable(tiles_torch)\n",
    "    if cuda: tiles_torch = tiles_torch.cuda(1)\n",
    "    z = tilenet.encode(tiles_torch)\n",
    "    if cuda: z = z.cpu()\n",
    "    z = z.data.numpy()\n",
    "    z_lst.append(z)\n",
    "embeddings = np.vstack(z_lst)\n",
    "embeddings.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show embedding on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it only if you want to delete the logs_dir\n",
    "!sudo rm -r {logs_dir}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "emb_pca = principalComponents = pca.fit_transform(embeddings)\n",
    "emb_pca_unique, indexes_unique = np.unique(emb_pca, axis=0, return_index=True)\n",
    "n_vis = 100  # number of image that will ne visualized\n",
    "vis_indexes = np.random.choice(indexes_unique, n_vis)\n",
    "label_im = np.moveaxis(tiles[vis_indexes].astype(int) / 255, -1, 1)\n",
    "writer = SummaryWriter(log_dir=logs_dir, comment='embedding_ermongroup')\n",
    "writer.add_embedding(torch.from_numpy(embeddings[vis_indexes]), label_img=torch.from_numpy(label_im), tag='emb_ermongroup')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 86567\n",
    "%tensorboard --logdir {logs_dir} --port=8115 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train many regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [svm.SVR(verbose=False)\n",
    "          , neighbors.KNeighborsRegressor(n_neighbors=10)\n",
    "          , LinearRegression()\n",
    "          , tree.DecisionTreeRegressor()\n",
    "          , GradientBoostingRegressor(verbose=False)\n",
    "          , AdaBoostRegressor()\n",
    "          , RandomForestRegressor(verbose=False)\n",
    "          , CatBoostRegressor(verbose=False)\n",
    "         ]\n",
    "pipe_dict = init_pipeline(models)\n",
    "task_handler = pipe_dict.pop('task_handler')\n",
    "pipe_dict = {k: v[:n_tiles] for k, v in pipe_dict.items()}\n",
    "locals().update(pipe_dict)\n",
    "features_without_geo_unique_coords = features_without_geo.values[unique_coords_idx]\n",
    "emb_unique_coords = embeddings[unique_coords_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat data to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_without_geo_unique_coords_with_emb = np.concatenate([emb_unique_coords, features_without_geo_unique_coords], axis=1)\n",
    "all_features_unique_coords_with_emb = np.concatenate([emb_unique_coords, all_features_unique_coords.values], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "emb_pca = principalComponents = pca.fit_transform(emb_unique_coords)\n",
    "features_without_geo_unique_coords_with_emb_pca = np.concatenate([emb_pca, features_without_geo_unique_coords], axis=1)\n",
    "all_features_unique_coords_with_emb_pca = np.concatenate([emb_pca, all_features_unique_coords.values], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data_dict and fit and score all the models on each dataset in data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_unique_coords = price[unique_coords_idx]\n",
    "data_dict = {\n",
    "    'all_features_unique_coords': (all_features_unique_coords, price_unique_coords)\n",
    "    , 'only_geo_features_unique_coords': (only_geo_features_unique_coords, price_unique_coords)\n",
    "    , 'features_without_geo_unique_coords': (features_without_geo.values[unique_coords_idx], price_unique_coords)\n",
    "    , 'features_without_geo_unique_coords_with_emb': (features_without_geo_unique_coords_with_emb, price_unique_coords)\n",
    "    , 'all_features_unique_coords_with_emb': (all_features_unique_coords_with_emb, price_unique_coords)\n",
    "    , 'features_without_geo_unique_coords_with_emb_pca': (features_without_geo_unique_coords_with_emb_pca, price_unique_coords)\n",
    "    , 'all_features_unique_coords_with_emb_pca': (all_features_unique_coords_with_emb_pca, price_unique_coords)\n",
    "            }\n",
    "\n",
    "print('start fitting:')\n",
    "results_df = fit_and_score_models_on_datasets(models, data_dict)\n",
    "results_df.columns.name = 'dataset'\n",
    "results_df.index.name = 'model'\n",
    "# print min_mae from results_df\n",
    "a, b = results_df.stack().idxmin()\n",
    "print(results_df.loc[[a], [b]])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find best performence model per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_df_argmin = results_df.idxmin(axis=0)\n",
    "best_model_df_min = results_df.min(axis=0)\n",
    "pd.DataFrame({'best performence model': best_model_df_argmin, 'mae': best_model_df_min})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find best performence dataset per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dataset_df_argmin = results_df.idxmin(axis=1)\n",
    "best_dataset_df_min = results_df.min(axis=1)\n",
    "pd.DataFrame({'best performence dataset': best_dataset_df_argmin, 'mae': best_dataset_df_min})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find best performence dataset per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dataset_df_argmin = results_df.idxmin(axis=1)\n",
    "best_dataset_df_min = results_df.min(axis=1)\n",
    "pd.DataFrame({'best performence dataset': best_dataset_df_argmin, 'mae': best_dataset_df_min})\n",
    "\n",
    "results_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_no_emb\n",
    "y = price\n",
    "\n",
    "def train_catboost(X, y):\n",
    "    N = y.shape[0]\n",
    "    indexes = np.arange(N)\n",
    "    np.random.seed()\n",
    "    np.random.shuffle(indexes)\n",
    "    train_ind, test_ind = train_test_split(indexes)\n",
    "    y_train, y_test = y[train_ind], y[test_ind]\n",
    "    X_train, X_test = X[train_ind], X[test_ind]\n",
    "    X_norm_train, X_norm_test, X_normalizer = my_z_score_norm(X_train, X_test, return_scalers=True)\n",
    "    args_tuple = ([CatBoostRegressor(verbose=False)], X_norm_train, y_train, X_norm_test, y_test)\n",
    "    model = train_models_from_splitted_data(*args_tuple)[0][0]\n",
    "    return model, X_normalizer, X_test, y_test\n",
    "\n",
    "model, X_normalizer, X_test, y_test = train_catboost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "def get_example_of_predictions(model, X_normalizer, X_test, y_test, n):\n",
    "    ind = np.random.randint(0, 4000, size=n)\n",
    "    y_pred = model.predict(X_normalizer.transform(X_test[ind])).astype(int)\n",
    "    gt = y_test.values[ind].astype(int)\n",
    "    df_dict = {'Prediction': y_pred, 'Truth': gt, 'Diff': np.abs(y_pred - gt)}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df\n",
    "    \n",
    "df = get_example_of_predictions(model, X_normalizer, X_test, y_test, n)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'all_features_unique_coords': (all_features_unique_coords, price[unique_coords_idx])\n",
    "    , 'only_geo_features_unique_coords': (only_geo_features_unique_coords, price[unique_coords_idx])\n",
    "    , 'features_without_geo_unique_coords': (features_without_geo.values[unique_coords_idx], price[unique_coords_idx])\n",
    "    , 'all_features': (all_features, price)\n",
    "    , 'features_without_geo': (features_without_geo, price)\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
