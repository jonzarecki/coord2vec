{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, os.path.join(parent_dir)) \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from coord2vec.models.data_loading.tile_features_loader import TileFeaturesDataset\n",
    "from coord2vec.models.model_utils import get_data_loader, get_pytorch_dataset\n",
    "from coord2vec.config import CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data_loader = get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([507385.0625,         nan,      0.0000,         nan,      0.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the NN\n",
    "from coord2vec.models.architectures import resnet18, multihead_model, dual_fc_head\n",
    "from coord2vec.models.losses import multihead_loss\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "z_dim = 128\n",
    "mtl_head_sizes = (3,2)\n",
    "n_channels = (data_loader.dataset[0][0].shape[0])\n",
    "\n",
    "model = resnet18(n_channels, z_dim)\n",
    "head1 = dual_fc_head(z_dim, n_classes=mtl_head_sizes[0])\n",
    "head2 = dual_fc_head(z_dim, n_classes=mtl_head_sizes[1])\n",
    "model = multihead_model(model, [head1, head2])\n",
    "\n",
    "# create the losses\n",
    "criterion = multihead_loss([nn.L1Loss(), nn.L1Loss()])\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "max_epochs = 2\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    for images_batch, features_batch in data_loader:\n",
    "        \n",
    "        # split the features into the multi_heads:\n",
    "        split_features_batch = torch.split(features_batch, mtl_head_sizes, dim=1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images_batch)[1]\n",
    "        loss = criterion(output, split_features_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = torch.ones(3,2,224,224)\n",
    "y_pred = model(x_pred)\n",
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-37.5586,  -5.7906,   3.3974,  17.2079,  -4.3377,  -5.1141, -21.1913,\n",
       "         42.6451,  24.8207,  14.1980, -38.6404, -26.9991, -22.4609,  -1.6728,\n",
       "          9.2489, -12.6510, -31.5708, -26.6453, -15.1955,  37.4996,  -2.7640,\n",
       "        -30.1221,  18.4570, -42.1786,  -1.1221, -14.3957,  39.2992, -31.2374,\n",
       "        -35.8495,  32.0851, -10.8533, -31.9567,  -8.2693, -30.8625,  39.6849,\n",
       "         13.4793,  -3.6356,  -0.7105, -41.6769,  -0.7229,   6.6207,  22.6575,\n",
       "         -5.6362,  35.7838, -41.1668, -34.6632,  23.2290, -14.5626, -38.9101,\n",
       "        -18.0895,  22.6260,  21.0566,  39.2145,   3.9535,  35.2733,   5.6688,\n",
       "          4.0585, -42.6405, -42.3857,  26.8344,  19.4617, -33.0107, -20.7948,\n",
       "         13.8923, -42.3280,  26.5748, -24.3154,   3.1902,  12.8675,  -5.0913,\n",
       "          3.5130, -32.5561,  -3.4316,   2.7310, -35.5863,  37.4442, -18.2592,\n",
       "         34.8666, -43.3461, -37.1842, -19.2268,  20.1802, -33.1238,   0.4628,\n",
       "         19.0287,  -2.6069,  -0.6670,  35.6660,  17.5473, -24.0009, -23.5042,\n",
       "         -3.4859,  -4.7559,  -4.5982,   8.3619,   6.7807,  25.2942,  11.9337,\n",
       "        -14.5166,  34.6736,  10.9396, -26.1660,  -0.2251,  -9.5810, -36.5018,\n",
       "        -34.1860,  37.1651,  30.0501,   0.0547, -34.3709, -37.5670, -32.0702,\n",
       "         -4.9931,  28.1766, -37.6617, -26.7699,  15.8240,  -2.2884, -34.8221,\n",
       "         11.3641, -38.6465, -33.9132,  -5.5768,  -8.2839,   7.6929,  40.9829,\n",
       "         31.1122,  22.4716], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = data_loader.dataset[0][0][None,:,:,:]\n",
    "y_pred = model(x_pred)\n",
    "y_pred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
